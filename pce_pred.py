# -*- coding: utf-8 -*-
"""3. PCE_pred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dTgmt6OfAop2YYILAMCyvYPRkXjhkgii

#Importing libraries and cleaned_df_Eg dataset
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

warnings.filterwarnings("ignore")
df=pd.read_csv('df_final.csv')
pd.set_option('display.max_columns', None)
df.head(2)

print(df.isna().sum().sum())

"""#Heatmap"""

df1 = df.iloc[:, :-1]
corr_matrix = df1.corr()

# Step 4: Draw the heatmap
plt.figure(figsize=(10, 8))  # optional: adjust figure size for readability
sns.heatmap(
    corr_matrix,
    annot=True,
    cmap='coolwarm',
    fmt=".2f",
    linewidths=0.5,
    annot_kws={"size": 6.5}
)

plt.savefig('Correlation.png', dpi=300, bbox_inches='tight')
plt.show()

# Define target columns (0,1,2,3)
target_cols = df1.iloc[:, [0, 1, 2, 3]]

# Compute correlation of target columns with all other features
corr_matrix = df1.corr().loc[target_cols.columns, :]

# Plot heatmap
plt.figure(figsize=(12, 3))  # wide format since we only need 4 rows
sns.heatmap(
    corr_matrix,
    annot=True,
    cmap='Blues',
    fmt=".2f",
    linewidths=0.5,
    annot_kws={"size": 7}
)

plt.title("Correlation of Target Variables with All Features", fontsize=12, pad=12)
plt.savefig('Correlation.png', dpi=300, bbox_inches='tight')
plt.show()

df1.describe()

"""#Data Prep"""

dfx=df.iloc[:,4:-1]
X=dfx.values
y=df.iloc[:,3].values
y=y.reshape(len(y),1)
dfx

"""#Ensemble Learning

##Random Forest
"""

from sklearn.ensemble import RandomForestRegressor
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.15, random_state=140)
rf=RandomForestRegressor(random_state=100)
rf.fit(X_train1,y_train1)

# Calculate cross-validated RMSE
mse_scores1 = cross_val_score(estimator=rf, X=X_train1, y=y_train1, cv=5, scoring='neg_mean_squared_error')
rmse_scores1 = np.sqrt(-mse_scores1)
print("RMSE: {:.4f}".format(rmse_scores1.mean()))
print("Standard Deviation: {:.4f}".format(rmse_scores1.std()))

#RF on Test set
y_pred1=rf.predict(X_test1)
y_pred1=y_pred1.reshape(len(y_pred1),1)
r_value1 = np.corrcoef(y_test1.squeeze(), y_pred1.squeeze())[0, 1]

# Calculate metrics on the test set
print("R2 on Test Set:", r2_score(y_test1, y_pred1))
print("R value:", r_value1)
print("MAE on Test Set:", mean_absolute_error(y_test1, y_pred1))
print("MSE on Test Set:", mean_squared_error(y_test1, y_pred1))
print("RMSE on Test Set:", np.sqrt(mean_squared_error(y_test1, y_pred1)))

#RF on Train set
y_train_pred1 = rf.predict(X_train1)
y_train_pred1=y_train_pred1.reshape(len(y_train_pred1),1)
r_value_p1 = np.corrcoef(y_train1.squeeze(), y_train_pred1.squeeze())[0, 1]

# Calculate metrics on the test set
print("R2 on Train Set:", r2_score(y_train1, y_train_pred1))
print("R value:", r_value_p1)
print("MAE on Train Set:", mean_absolute_error(y_train1, y_train_pred1))
print("MSE on Train Set:", mean_squared_error(y_train1, y_train_pred1))
print("RMSE on Train Set:", np.sqrt(mean_squared_error(y_train1, y_train_pred1)))

np.set_printoptions(precision=2, suppress=True)
print(np.concatenate((y_pred1.reshape(len(y_pred1),1), y_test1.reshape(len(y_test1),1)),1))

# Plotting the results for the training set
plt.figure(figsize=(4.5, 4))

plt.plot(y_train1, y_train1, color='black', label='Actual Values')
plt.scatter(y_train1, y_train_pred1, color='royalblue', label='Train set: 0.635 RMSE')
plt.scatter(y_test1, y_pred1, color='deeppink', label='Test set: 1.191 RMSE')

plt.xlabel('Actual PCE (%)')
plt.ylabel('Predicted PCE (%)')
plt.legend()
#plt.grid(True)
plt.title('RF Model: Actual vs Predicted PCE (%)')
plt.savefig('RF_PCE.png', dpi=300, bbox_inches='tight')

plt.tight_layout()
plt.show()

"""##Gradient Boosting"""

from sklearn.ensemble import GradientBoostingRegressor
X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.15,  random_state=140)
gbr=GradientBoostingRegressor(random_state=100)
gbr.fit(X_train2, y_train2)

# Calculate cross-validated RMSE
mse_scores2 = cross_val_score(estimator=gbr, X=X_train2, y=y_train2, cv=5, scoring='neg_mean_squared_error')
rmse_scores2 = np.sqrt(-mse_scores2)
print("RMSE: {:.4f}".format(rmse_scores2.mean()))
print("Standard Deviation: {:.4f}".format(rmse_scores2.std()))

#GBR on test set
y_pred2=gbr.predict(X_test2)
y_pred2=y_pred2.reshape(len(y_pred2),1)
r_value2 = np.corrcoef(y_test2.squeeze(), y_pred2.squeeze())[0, 1]

# Calculate metrics on the test set
print("R2 on Test Set:", r2_score(y_test2, y_pred2))
print("R value:", r_value2)
print("MAE on Test Set:", mean_absolute_error(y_test2, y_pred2))
print("MSE on Test Set:", mean_squared_error(y_test2, y_pred2))
print("RMSE on Test Set:", np.sqrt(mean_squared_error(y_test2, y_pred2)))

#GBR on train set
y_train_pred2 = gbr.predict(X_train2)
y_train_pred2=y_train_pred2.reshape(len(y_train_pred2),1)
r_value_p2 = np.corrcoef(y_train2.squeeze(), y_train_pred2.squeeze())[0, 1]

# Calculate metrics on the test set
print("R2 on Test Set:", r2_score(y_train2, y_train_pred2))
print("R value:", r_value_p2)
print("MAE on Test Set:", mean_absolute_error(y_train2, y_train_pred2))
print("MSE on Test Set:", mean_squared_error(y_train2, y_train_pred2))
print("RMSE on Test Set:", np.sqrt(mean_squared_error(y_train2, y_train_pred2)))

np.set_printoptions(precision=2,suppress=True)
print(np.concatenate((y_pred2.reshape(len(y_pred2),1), y_test2.reshape(len(y_test2),1)),1))

# Plotting the results for the training set
plt.figure(figsize=(4.5, 4))
plt.plot(y_train2, y_train2, color='black', label='Actual Values')
plt.scatter(y_train2, y_train_pred2, edgecolors='royalblue',
                linewidth=1.2,color='royalblue', label='Train set: 0.999 RMSE')
plt.scatter(y_test2, y_pred2, edgecolors='deeppink',
                linewidth=1.2,color='deeppink', label='Test set: 1.156 RMSE')
plt.xlabel('Actual PCE (%)')
plt.ylabel('Predicted PCE (%)')
plt.legend()
#plt.grid(True)
plt.title('GBR Model: Actual vs Predicted PCE (%)')
plt.savefig('GBR_PCE.png', dpi=300, bbox_inches='tight')


plt.tight_layout()
plt.show()

"""##XGBoost"""

import xgboost as xgb
X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.15, random_state=140)
xgb = xgb.XGBRegressor(random_state=42)
xgb.fit(X_train3,y_train3)

mse_scores3 = cross_val_score(estimator = xgb, X = X_train3, y = y_train3, cv = 5, scoring='neg_mean_squared_error')
rmse_scores3 = np.sqrt(-mse_scores3)
print("RMSE: {:.4f}".format(rmse_scores3.mean()))
print("Standard Deviation: {:.4f}".format(rmse_scores3.std()))

#XGB on Test set
y_pred3=xgb.predict(X_test3)
y_pred3=y_pred3.reshape(len(y_pred3),1)
r_value3 = np.corrcoef(y_test3.squeeze(), y_pred3.squeeze())[0, 1]

# Calculate metrics on the test set
print("R2 on Test Set:", r2_score(y_test3, y_pred3))
print("R value:", r_value3)
print("MAE on Test Set:", mean_absolute_error(y_test3, y_pred3))
print("RMSE on Test Set:", np.sqrt(mean_squared_error(y_test3, y_pred3)))

#XGB on Train set
y_train_pred3 = xgb.predict(X_train3)

y_train_pred3=y_train_pred3.reshape(len(y_train_pred3),1)
r_value_p3 = np.corrcoef(y_train3.squeeze(), y_train_pred3.squeeze())[0, 1]

# Calculate metrics on the test set
print("R2 on Train Set:", r2_score(y_train3, y_train_pred3))
print("R value:", r_value_p3)
print("MAE on Train Set:", mean_absolute_error(y_train3, y_train_pred3))
print("MSE on Train Set:", mean_squared_error(y_train3, y_train_pred3))
print("RMSE on Train Set:", np.sqrt(mean_squared_error(y_train3, y_train_pred3)))

print(np.concatenate((y_pred3.reshape(len(y_pred3),1), y_test3.reshape(len(y_test3),1)),1))

y_train_pred3 = xgb.predict(X_train3)
# Plotting the results for the training set
plt.figure(figsize=(4.5, 4))

plt.plot(y_train3, y_train3, color='black', label='Actual Values')
plt.scatter(y_train3, y_train_pred3, color='royalblue', label='Train set: 0.138 RMSE')
plt.scatter(y_test3, y_pred3, color='deeppink', label='Test set: 1.169 RMSE')

plt.title('Training Set: Actual vs Predicted')
plt.xlabel('Actual PCE (%)')
plt.ylabel('Predicted PCE (%)')
plt.legend()
#plt.grid(True)
plt.title('XGB Model: Actual vs Predicted PCE (%)')
plt.savefig('XGB_PCE.png', dpi=300, bbox_inches='tight')

plt.tight_layout()
plt.show()

"""##CatBoost"""

!pip install catboost

from catboost import CatBoostRegressor
X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.15, random_state=140)
catboost = CatBoostRegressor(random_state=42)
catboost.fit(X_train4, y_train4, verbose=0)

# Evaluating with cross-validation
mse_scores4 = cross_val_score(estimator = catboost, X = X_train4, y = y_train4, cv = 5, scoring='neg_mean_squared_error')
rmse_scores4 = np.sqrt(-mse_scores4)
print("RMSE: {:.4f}".format(rmse_scores4.mean()))
print("Standard Deviation: {:.4f}".format(rmse_scores4.std()))

#CB on Test set
y_pred4 = catboost.predict(X_test4)
y_pred4 = y_pred4.reshape(len(y_pred4), 1)
r_value4 = np.corrcoef(y_test4.squeeze(), y_pred4.squeeze())[0, 1]

# Calculate metrics on the test set
print("R2 on Test Set:", r2_score(y_test4, y_pred4))
print("R value:", r_value4)
print("MAE on Test Set:", mean_absolute_error(y_test4, y_pred4))
print("MSE on Test Set:", mean_squared_error(y_test4, y_pred4))
print("RMSE on Test Set:", np.sqrt(mean_squared_error(y_test4, y_pred4)))

#CB on Train set
y_train_pred4 = catboost.predict(X_train4)

y_train_pred4=y_train_pred4.reshape(len(y_train_pred4),1)
r_value_p4 = np.corrcoef(y_train4.squeeze(), y_train_pred4.squeeze())[0, 1]

# Calculate metrics on the test set
print("R2 on Train Set:", r2_score(y_train4, y_train_pred4))
print("R value:", r_value_p4)
print("MAE on Train Set:", mean_absolute_error(y_train4, y_train_pred4))
print("MSE on Train Set:", mean_squared_error(y_train4, y_train_pred4))
print("RMSE on Train Set:", np.sqrt(mean_squared_error(y_train4, y_train_pred4)))

np.set_printoptions(precision=2, suppress=True)
print(np.concatenate((y_pred4.reshape(len(y_pred4),1), y_test4.reshape(len(y_test4),1)),1))

# Plotting the results for the training set
plt.figure(figsize=(4.5, 4))

plt.plot(y_train4, y_train4, color='black', label='Actual Values')
plt.scatter(y_train4, y_train_pred4, color='royalblue', label='Train Set: 0.497 RMSE')
plt.scatter(y_test4, y_pred4, color='deeppink', label='Test set: 1.095 RMSE')

plt.title('Training Set: Actual vs Predicted')
plt.xlabel('Actual PCE (%)')
plt.ylabel('Predicted PCE (%)')
plt.legend()
#plt.grid(True)
plt.title('CB Model: Actual vs Predicted PCE (%)')
plt.savefig('CB_PCE.png', dpi=300, bbox_inches='tight')

plt.tight_layout()
plt.show()

"""#SHAP Analysis"""

pip install shap

import shap
shap.initjs()

dfx.shape

col=dfx.columns
dfx_test = pd.DataFrame(data=X, columns=col)

explainer=shap.Explainer(catboost)
shap_values=explainer(dfx_test[0:])

np.shape(shap_values.values)

#plt.figure(figsize=(10, 3))
shap.summary_plot(shap_values, plot_type='violin')
# Create the summary plot
#shap.summary_plot(shap_values)

plt.savefig("shap_summary.png", dpi=300, bbox_inches="tight")

plt.show()

plt.figure(figsize=(10, 4))
shap.summary_plot(
    shap_values,
    plot_type="violin",   # beeswarm style
    max_display=7
)

plt.savefig("shap_summary.png", dpi=300, bbox_inches="tight")
plt.show()

shap.summary_plot(shap_values, plot_type='violin',max_display=7, show=False)

plt.savefig("shap_summary.png", dpi=300, bbox_inches="tight")
plt.close()

plt.figure(figsize=(5, 3))
shap.plots.bar(shap_values)
plt.show()

shap.initjs()
shap.force_plot(shap_values[297])

shap.plots.waterfall(shap_values[297], max_display=10)

shap.plots.scatter(shap_values[:,'BG'],color=shap_values[:,'Sn'], show=False, hist=False)

# set axis limits according to your requirement
plt.xlim(1.1, 2.6)
#plt.ylim(-1, 1)

plt.savefig("FD_BG.png", dpi=300, bbox_inches="tight")
plt.close()

# Scatter plot without background distribution
shap.plots.scatter(shap_values[:, 'BG'],color=shap_values[:,'Sn'], show=False, hist=False)

# Set axis limits
plt.xlim(1.1, 2.6)
# plt.ylim(-1, 1)

plt.show()

shap.plots.scatter(shap_values[:,'BG'],color=shap_values[:,'Sn'], show=False, hist=False)

# set axis limits according to your requirement
plt.xlim(1.1, 2.6)
#plt.ylim(-1, 1)

plt.savefig("FD_BG.png", dpi=300, bbox_inches="tight")
plt.close()

shap.plots.scatter(shap_values[:,'CBO'],color=shap_values[:,'BG'], show=False, hist=False)
plt.show()

shap.plots.scatter(shap_values[:,'CBO'],color=shap_values[:,'BG'], show=False, hist=False)

plt.savefig("FD_CBO.png", dpi=300, bbox_inches="tight")
plt.close()

shap.plots.scatter(shap_values[:,'VBO'],color=shap_values[:,'BG'], hist=False)
plt.show()

shap.plots.scatter(shap_values[:,'VBO'],color=shap_values[:,'BG'], show=False, hist=False)
plt.savefig("FD_VBO.png", dpi=300, bbox_inches="tight")
plt.close()

shap.plots.scatter(shap_values[:,'FA'],color=shap_values[:,'BG'], hist=False)

shap.plots.scatter(shap_values[:,'FA'],show=False,color=shap_values[:,'BG'], hist=False)
plt.savefig("FD_FA.png", dpi=300, bbox_inches="tight")
plt.close()

shap.plots.scatter(shap_values[:,'Cs'], color=shap_values[:,'FA'], hist=False)

shap.plots.scatter(shap_values[:,'Cs'], color=shap_values[:,'FA'], show=False, hist=False)
plt.savefig("FD_Cs.png", dpi=300, bbox_inches="tight")
plt.close()

shap.plots.scatter(shap_values[:,'MA'],color=shap_values[:,'FA'], hist=False)

shap.plots.scatter(shap_values[:,'MA'], color=shap_values[:,'FA'], show=False, hist=False)
plt.savefig("FD_MA.png", dpi=300, bbox_inches="tight")
plt.close()

shap.plots.scatter(shap_values[:,'Sn'], show=False, hist=False)

shap.plots.scatter(shap_values[:,'Sn'], show=False, hist=False)
plt.savefig("FD_Sn.png", dpi=300, bbox_inches="tight")
plt.close()

shap.plots.scatter(shap_values[:,'h_mobility'], hist=False)

shap.plots.scatter(shap_values[:,'h_mobility'], show=False, hist=False)
plt.savefig("FD_h.png", dpi=300, bbox_inches="tight")
plt.close()

# your scatter plot
shap.plots.scatter(shap_values[:, 'e_mobility'], show=False, hist=False)

# set axis limits according to your requirement
#plt.xlim(-0.0001, 0.007)   # example: x-axis from -2 to 2
#plt.ylim(-1, 1)   # example: y-axis from -1 to 1

plt.show()

shap.plots.scatter(shap_values[:,'e_mobility'], show=False, hist=False)
plt.savefig("FD_e.png", dpi=300, bbox_inches="tight")
plt.close()

shap.plots.scatter(shap_values[:,'Br'],color=shap_values[:,'BG'], hist=False)

shap.plots.scatter(shap_values[:,'Br'],color=shap_values[:,'BG'],show=False, hist=False)
plt.savefig("FD_Br.png", dpi=300, bbox_inches="tight")
plt.close()

shap.plots.scatter(shap_values[:,'I'],color=shap_values[:,'BG'], hist=False)

shap.force_plot(shap_values[298])

"""# Validate"""

features = [
    "Cs", "MA", "FA", "Pb", "Sn", "I", "Br", "BG", "CBO", "VBO",
    "arch_nip", "arch_pin", "h_mobility", "e_mobility",
    "CBM", "VBM", "ETL_CBM", "HTL_VBM"
]

# ---- Manually enter datapoint values here ----
datapoint1 = [0.17, 0, 0.83, 1.0, 0.0, 2.7, 0.3, 1.62, -0.47, 0.5,
              0, 1, 0.0000832, 0.00008, -4.03, -5.65, -4.5, -5.15]

datapoint2 = [0.17, 0, 0.83, 1.0, 0.0, 2.7, 0.3, 1.62, -0.47, 0.5,
              0, 1, 0.0000697, 0.00008, -4.03, -5.65, -4.5, -5.15]
datapoint3 = [0.17,	0,	0.83,	1,	0,	3,	0,	1.58,	-0.1,	0.58,	0,	1,	1.20E-03,	4.00E-04,	-4.1,	-5.68,	-4.2,	-5.1]

datapoint4 = [1,	0,	0,	1,	0,	0,	3,	2.36,	-1.11,	0.25,	1,	0,	3.69E-05,	9.92E-04,	-3.32,	-5.68,	-4.43,	-5.43]
# Convert into DataFrame
X_manual = pd.DataFrame([datapoint1, datapoint2, datapoint3, datapoint4], columns=features)

# Predict
preds = catboost.predict(X_manual)
preds

